{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVdtcEDEW9hU"
   },
   "source": [
    "# Notebook App for Data Enrichment Using LangGrpah and Tavily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yd-1RKEnW9hV"
   },
   "source": [
    "## Introduction\n",
    "Welcome to the Data Enrichment Sheet!\n",
    "\n",
    "This sheet is designed to enhance your data by populating missing values using advanced search capabilities from Tavily and the LangGraph Framework. It accepts either a CSV or Excel file path or a Google Sheet name (follow the instructions in the \"Google Colab Set Up\" section).\n",
    "\n",
    "For Google Sheets and Excel files, the agent will create an additional sheet containing the populated data table. For CSV files, a new CSV file will be generated with the enriched data provided by the agent.\n",
    "\n",
    "For detailed instructions on how to use this sheet and call the agent to start enriching your data, please refer to the \"Call Agent\" section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caex2CjJW9hW"
   },
   "source": [
    "## Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFRvBW_uapc2",
    "outputId": "5d86347c-12ee-4062-992b-50b94e3b44f4"
   },
   "outputs": [],
   "source": [
    "!pip install langgraph tavily-python openai openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fb5brO4BW9hW"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luqDjbJNW9hW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import asyncio\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from typing import TypedDict, Annotated\n",
    "from openai import AsyncOpenAI\n",
    "from tavily import AsyncTavilyClient\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y47hwJmg21I"
   },
   "source": [
    "## Google Colab Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxH73q2Gz1Rd"
   },
   "source": [
    "To connent with Google Sheets upload this notebook to Google Colab. Uncomment the following lines and ensure to input your Google Sheet name for your agent in the Call Agent section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nh7mcADKhAdu"
   },
   "outputs": [],
   "source": [
    "# import gspread # Python API for Google Sheets\n",
    "# from google.colab import auth\n",
    "# from google.auth import default\n",
    "\n",
    "# # Authenticate and create the gspread client\n",
    "# auth.authenticate_user()\n",
    "# creds, _ = default()\n",
    "\n",
    "# gc = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1a8tvFOW9hX"
   },
   "source": [
    "## Build Master Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHcXyoX6Y2dz"
   },
   "source": [
    "### Master's Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vzxz1mKhYzZN"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "  raw_data: pd.DataFrame                  # original panda data frame\n",
    "  new_data: pd.DataFrame                  # populated panda data frame\n",
    "  colab: str                              # google sheet name\n",
    "  excel: str                              # path to Excel file\n",
    "  csv: str                                # path to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUJuVVjV70y9"
   },
   "source": [
    "### Master's Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7uYJ337Ymgw"
   },
   "outputs": [],
   "source": [
    "class DataEnrichmentAgent():\n",
    "  def __init__(self):\n",
    "    # Initialize agents\n",
    "    data_agent = DataAgent()\n",
    "    enrich_agent = EnrichAgent()\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    workflow.add_node(\"get_data\", data_agent.get_df)\n",
    "    workflow.add_node(\"enrich_data\", enrich_agent.run)\n",
    "    workflow.add_node(\"write_data\", data_agent.write_df)\n",
    "\n",
    "    workflow.set_entry_point(\"get_data\")\n",
    "\n",
    "    workflow.add_edge(\"get_data\", \"enrich_data\")\n",
    "    workflow.add_edge(\"enrich_data\", \"write_data\")\n",
    "    workflow.add_edge(\"write_data\", END)\n",
    "\n",
    "    self.workflow = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4HoV9Ds8RmL"
   },
   "source": [
    "## Sub Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SBektD48VjA"
   },
   "source": [
    "### Data Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7MdfXoS8cpP"
   },
   "outputs": [],
   "source": [
    "class DataAgent():\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  # Google Sheets Integration through Colab\n",
    "  def connect_to_google_sheets_colab(self, sheet_name):\n",
    "    sheet = gc.open(sheet_name).sheet1\n",
    "    data = sheet.get_all_records()\n",
    "    df = pd.DataFrame(data)\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "  def get_df(self, state: AgentState):\n",
    "    # Function to get Data Frame and store in state\n",
    "    if state['colab']:\n",
    "      df = self.connect_to_google_sheets_colab(state['colab'])\n",
    "      return {\"raw_data\": df}\n",
    "    elif state['csv']:\n",
    "      df = pd.read_csv(state['csv'])\n",
    "      return {\"raw_data\": df}\n",
    "    elif state['excel']:\n",
    "      df = pd.read_excel(state['excel'])\n",
    "      return {\"raw_data\": df}\n",
    "    else:\n",
    "        print(\"Either colab or csv or excel argument is required.\")\n",
    "        return\n",
    "\n",
    "  def write_df(self, state: AgentState):\n",
    "    df = state['new_data']\n",
    "    if state['colab']:\n",
    "        # Use existing Colab authentication (assuming `gc` is a gspread client)\n",
    "        spreadsheet = gc.open(state['colab'])\n",
    "\n",
    "        # Add a new sheet\n",
    "        sheet_name = f\"AgentSheet{len(spreadsheet.worksheets()) + 1}\"\n",
    "        spreadsheet.add_worksheet(title=sheet_name, rows=str(df.shape[0]), cols=str(df.shape[1]))\n",
    "        new_sheet = spreadsheet.worksheet(sheet_name)\n",
    "\n",
    "        # Write DataFrame to the new sheet\n",
    "        new_sheet.update([df.columns.values.tolist()] + df.values.tolist())\n",
    "        print(f\"Data written to new sheet '{sheet_name}' in Google Sheets document '{state['colab']}'.\")\n",
    "\n",
    "    elif state['excel']:\n",
    "      # Load the existing workbook\n",
    "      wb = load_workbook(state['excel'])\n",
    "\n",
    "      # Generate a new sheet name based on existing sheets count\n",
    "      sheet_name = f\"AgentSheet{len(wb.sheetnames) + 1}\"\n",
    "\n",
    "      # Create a new sheet and write data\n",
    "      ws = wb.create_sheet(title=sheet_name)\n",
    "\n",
    "      # Write DataFrame to the new sheet\n",
    "      for r in dataframe_to_rows(df, index=False, header=True):\n",
    "        ws.append(r)\n",
    "\n",
    "      # Save changes to the Excel file\n",
    "      wb.save(state['excel'])\n",
    "      wb.close()\n",
    "      print(f\"Data written to new sheet '{sheet_name}' in Excel document '{state['excel']}'.\")\n",
    "\n",
    "    elif state['csv']:\n",
    "      # Extract file name from the csv path and construct new name\n",
    "      file_name = os.path.basename(state['csv'])\n",
    "      # Output DataFrame to a CSV file\n",
    "      output_csv = f\"AgentSheet{file_name}\" # construct new file path\n",
    "      df.to_csv(output_csv, index=False)\n",
    "      print(f\"Data written to CSV file '{output_csv}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_lNj1cFFvZF"
   },
   "source": [
    "### Enrich Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5EeOw18bj4J"
   },
   "outputs": [],
   "source": [
    "# Colors\n",
    "RED = '\\033[91m'\n",
    "GREEN = '\\033[92m'\n",
    "BLUE = '\\033[94m'\n",
    "YELLOW = '\\033[93m'\n",
    "ENDC = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wkkeSPBF2Oy"
   },
   "outputs": [],
   "source": [
    "class EnrichAgent():\n",
    "  def __init__(self):\n",
    "    # Maximum number of columns to try to resolve at once\n",
    "    self.MAX_COLS_PER_PASS = 5\n",
    "\n",
    "    # Maximum number of fill-in passes to attempt\n",
    "    self.MAX_PASSES = 5\n",
    "\n",
    "  async def generate_search_query(self, head, columns):\n",
    "    \"\"\"\n",
    "    This function takes the head of the table (all column names) and the names of the columns that it needs to fill in, and generates a search prompt for Tavily\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a researcher with the task of filling in a spreadsheet. The spreadsheet contains the columns {str(head)} and {str(columns)} have not been filled in yet.\n",
    "    Write a web search query for a search engine that you will use to fill in each row of the spreadsheet one by one. In the query, replace the name of the entry you are researching with $ENTRY\n",
    "    Respond only with the query.\n",
    "    \"\"\"\n",
    "\n",
    "    response = await openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip('\\'\"')\n",
    "\n",
    "  async def fill_in_row(self, df, head, row_index, columns, search_query):\n",
    "    \"\"\"\n",
    "    This function takes a data frame, a row to complete, a list of column names to complete and the query to search.\n",
    "    It calls the Tavily API to retrieve information using the search_query and prompts OpenAI to extract column values from the response\n",
    "    \"\"\"\n",
    "    # if the search_query is a coroutine, await it\n",
    "    if asyncio.iscoroutine(search_query):\n",
    "        search_query = await search_query\n",
    "\n",
    "    entry = df.iloc[row_index][head[0]]\n",
    "\n",
    "    search_query = search_query.replace(\"$ENTRY\", entry)\n",
    "    print(f\"{BLUE}Tavily Search: {search_query}{ENDC}\")\n",
    "    tavily_response = await tavily.search(search_query)\n",
    "\n",
    "    # Only save the title and content from each Tavily result\n",
    "    research_results = [{'title': r['title'], 'content': r['content']} for r in tavily_response['results']]\n",
    "\n",
    "    # To ensure consistency with the existing data, come up with an example based on existing data in the df\n",
    "    example = {}\n",
    "    for col in columns:\n",
    "      for i in range(len(df)):\n",
    "        if i != row_index and not pd.isna(df.iloc[i][col]):\n",
    "          example[col] = df.iloc[i][col]\n",
    "          break\n",
    "\n",
    "    example_str = f'Example: {str(example)}\\n' if len(example) > 0 else ''\n",
    "\n",
    "    prompt = f\"You are filling in a spreadsheet using online sources. The following fields need to be filled in for the entry '{entry}': {', '.join(columns)}\\n\" \\\n",
    "              \"Using the data below, find values for as many of these fields as you can. Reply with a JSON object, linking each field to its value.\\n\" \\\n",
    "            f\"{example_str}\" \\\n",
    "              \"Values must be written in a spreadsheet friendly format. If the data is not sufficient to complete a field, simply omit it from the result object.\\n\" \\\n",
    "              \"Do not incude any text other than the JSON object in your response.\\n\\n\" \\\n",
    "                      f\"{str(research_results)}\"\n",
    "\n",
    "    response = await openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    json_str = response.choices[0].message.content\n",
    "    try:\n",
    "      fields = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "      print(f\"{RED}Failed to parse JSON: {json_str}{ENDC}\")\n",
    "      return\n",
    "\n",
    "    for field, value in fields.items():\n",
    "      if field not in columns:\n",
    "        print(f\"{RED}LLM returned an invalid field '{field}'{ENDC}\")\n",
    "        continue\n",
    "\n",
    "      col_index = df.columns.get_loc(field)\n",
    "      if not pd.isna(df.iloc[row_index][field]):\n",
    "        print(f\"{YELLOW}Skipping field '{field}' as it already has a value:{ENDC}\", df.iloc[row_index][field])\n",
    "        continue\n",
    "\n",
    "      df.at[row_index, field] = str(value)\n",
    "      print(f\"{GREEN}Filled in field '{field}' with value '{value}'{ENDC}\")\n",
    "\n",
    "  async def run(self, state: AgentState):\n",
    "    \"\"\"\n",
    "    Main Function:\n",
    "    First, we try to fill in the maximum number of column in one pass for each row. We then loop and try to fill in the missing columns\n",
    "    \"\"\"\n",
    "    df = state['raw_data'].copy()\n",
    "    head = list(df.columns)\n",
    "    # First pass: try to fill in as much as possible\n",
    "    general_query = await self.generate_search_query(head, head[1:self.MAX_COLS_PER_PASS+1])\n",
    "\n",
    "    coroutines = []\n",
    "    for row_index in range(len(df)):\n",
    "      has_missing_columns = False\n",
    "      for col_index in range(len(head)):\n",
    "        if pd.isna(df.iloc[row_index][head[col_index]]):\n",
    "          has_missing_columns = True\n",
    "          break\n",
    "\n",
    "\n",
    "      if not has_missing_columns:\n",
    "        print(f\"{YELLOW}No missing columns for row {row_index}{ENDC}\")\n",
    "        continue\n",
    "\n",
    "      coroutines.append(self.fill_in_row(df, head, row_index, head[1:], general_query))\n",
    "\n",
    "    await asyncio.gather(*coroutines)\n",
    "\n",
    "    # Now, fill in the missing fields for each row\n",
    "    for _ in range(self.MAX_PASSES):\n",
    "      all_completed = True\n",
    "\n",
    "      coroutines = []\n",
    "      for row_index in range(len(df)):\n",
    "        missing_columns = []\n",
    "        for col_index in range(len(head)):\n",
    "          if pd.isna(df.iloc[row_index][head[col_index]]) and len(missing_columns) < self.MAX_COLS_PER_PASS:\n",
    "            missing_columns.append(head[col_index])\n",
    "\n",
    "        if len(missing_columns) > 0:\n",
    "          all_completed = False\n",
    "          print(f\"Missing columns for row {row_index}: {', '.join(missing_columns)}\")\n",
    "          query = self.generate_search_query(head, missing_columns)\n",
    "          coroutines.append(self.fill_in_row(df, head, row_index, missing_columns, query))\n",
    "        else:\n",
    "          print(f\"No missing columns for row {row_index}\")\n",
    "\n",
    "      if all_completed:\n",
    "        break\n",
    "\n",
    "      await asyncio.gather(*coroutines)\n",
    "\n",
    "    # Check if everything was completed\n",
    "    all_completed = True\n",
    "    for row_index in range(len(df)):\n",
    "      for col_index in range(len(head)):\n",
    "        if pd.isna(df.iloc[row_index][head[col_index]]):\n",
    "          all_completed = False\n",
    "          break\n",
    "\n",
    "    if all_completed:\n",
    "      print(f\"{GREEN}All rows completed{ENDC}\")\n",
    "    else:\n",
    "      print(f\"{RED}Not all rows completed{ENDC}\")\n",
    "    return {\"new_data\": df}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbFQs7Mxw8S8"
   },
   "source": [
    "# Call Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y84CPSSY5R7v"
   },
   "outputs": [],
   "source": [
    "# Set Your API Keys\n",
    "OPENAI_API_KEY = \"YOUR OPENAI API KEY\"\n",
    "TAVILY_API_KEY = \"YOUR TAIVLY API KEY\"\n",
    "\n",
    "openai = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "tavily = AsyncTavilyClient(TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yEEPnCm0p_I",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the input data for your agent, specifying the path to either a CSV file or an Excel file,\n",
    "# and optionally the name of your Google Sheet\n",
    "csv_path = \"your_csv_path\"       #E.g \"Desktop/my_csv.scv\"\n",
    "excel_path = \"test.xlsx\"         #E.g \"Desktop/my_exl.xlsx\"\n",
    "colab = \"google_sheet_name\"      #E.g \"My_Google_Sheet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "kmOGMPynhQy4",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "eefa3c8b-08ef-444e-b1c0-64e55d538f25"
   },
   "outputs": [],
   "source": [
    "my_agent = DataEnrichmentAgent()\n",
    "input = {\n",
    "    # Example: Uncomment and specify the path to your CSV file above\n",
    "    # \"csv\": csv_path,\n",
    "\n",
    "    # Example: Uncomment and specify the path to your Excel file above\n",
    "    \"excel\": excel_path,\n",
    "\n",
    "    # Example: Uncomment and specify your Google Sheet name above\n",
    "    # \"colab\": colab\n",
    "}\n",
    "result = await my_agent.workflow.ainvoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8q8ma2sHihs"
   },
   "source": [
    "#### Your original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "W-29kkqNHbIS",
    "outputId": "3a08a2b4-3ce6-4570-b390-8b7971193dbd"
   },
   "outputs": [],
   "source": [
    "result['raw_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGBOjt-8HqUp"
   },
   "source": [
    "#### Enriched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "fqvsjM2sHdvd",
    "outputId": "3f046137-6e9d-4dbb-a163-1691bca7d2f6"
   },
   "outputs": [],
   "source": [
    "result['new_data']"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "colab": {
   "collapsed_sections": [
    "v1a8tvFOW9hX",
    "XUJuVVjV70y9",
    "2SBektD48VjA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
